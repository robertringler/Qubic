"""QRATUM Discovery Engine - Unified Vulnerability Science (Defensive Only)

This module implements a non-exploitative scientific vulnerability discovery system
that identifies, quantifies, and mitigates latent structural vulnerabilities across
the QRATUM stack.

The Discovery Engine operates in defensive-only mode, focusing on:
- Subsystem enumeration and dependency analysis
- Latent risk field construction
- Entropy and blind-zone detection
- Coupling drift surveillance
- Phase-transition modeling
- Invariant failure signature extraction
- Defensive mitigation architecture generation

No exploit paths, penetration methods, or evasion tactics are generated.
"""

import json
import math
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple


class SubsystemType(Enum):
    """Types of subsystems in QRATUM."""

    GENOMICS_PIPELINE = "genomics_pipeline"
    AION_RUNTIME = "aion_runtime"
    DATA_INGESTION = "data_ingestion"
    ORCHESTRATION = "orchestration"
    GOVERNANCE = "governance"
    ECONOMIC_CONTROL = "economic_control"
    MERKLE_CHAIN = "merkle_chain"
    CONTRACT_ENGINE = "contract_engine"
    QUANTUM_KERNEL = "quantum_kernel"
    MEMORY_MANAGER = "memory_manager"
    SCHEDULER = "scheduler"


class InterfaceType(Enum):
    """Types of interfaces between subsystems."""

    DATA_FLOW = "data_flow"
    CONTROL_FLOW = "control_flow"
    EVENT_BUS = "event_bus"
    RPC_CALL = "rpc_call"
    SHARED_MEMORY = "shared_memory"
    MESSAGE_QUEUE = "message_queue"


@dataclass
class Subsystem:
    """Represents a subsystem in the QRATUM stack."""

    subsystem_id: str
    name: str
    subsystem_type: SubsystemType
    description: str
    interfaces: List[str] = field(default_factory=list)
    dependencies: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __hash__(self):
        return hash(self.subsystem_id)


@dataclass
class Interface:
    """Represents an interface between subsystems."""

    interface_id: str
    name: str
    interface_type: InterfaceType
    source_subsystem: str
    target_subsystem: str
    protocol: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LatentRiskMetrics:
    """Latent risk metrics for a subsystem.

    Attributes:
        defect_density (ρ): Estimated latent defects per unit of complexity
        resilience_elasticity (Er): System's ability to recover from perturbations
        coupling_volatility (σC): Variance in inter-module coupling strength
        recovery_half_life (τ): Time for system to recover 50% after disruption
    """

    defect_density: float  # ρ
    resilience_elasticity: float  # Er
    coupling_volatility: float  # σC
    recovery_half_life: float  # τ (seconds)
    entropy_local: float  # Local entropy H_local
    entropy_average: float  # Average entropy H̄


@dataclass
class EntropyBottleneck:
    """Represents a structural blind-zone with high entropy."""

    subsystem_id: str
    entropy_ratio: float  # H_local / H̄
    bottleneck_type: str  # data_flow, execution, governance, genomic
    severity: str  # LOW, MEDIUM, HIGH, CRITICAL
    description: str


@dataclass
class CouplingDrift:
    """Tracks time-derivative of inter-module coupling."""

    source_id: str
    target_id: str
    coupling_strength: float
    drift_rate: float  # dC/dt
    timestamp: str
    is_metastable: bool


@dataclass
class PhaseTransitionRisk:
    """Phase transition risk assessment."""

    system_health: float  # m(t) = 1 - L(t)/L_crit
    variance_spike: bool
    entropy_localization: bool
    resilience_compression: bool
    collapse_probability: float
    precursor_signals: List[str]


@dataclass
class VulnerabilityMetricsReport:
    """Complete vulnerability metrics report."""

    timestamp: str
    subsystems: List[Subsystem]
    metrics: Dict[str, LatentRiskMetrics]
    entropy_bottlenecks: List[EntropyBottleneck]
    coupling_drifts: List[CouplingDrift]
    phase_risk: PhaseTransitionRisk
    invariant_signatures: List[str]
    mitigation_recommendations: List[str]


class DependencyGraph:
    """Dependency graph G(V,E) where edges represent functional coupling."""

    def __init__(self):
        self.vertices: Dict[str, Subsystem] = {}
        self.edges: List[Tuple[str, str, float]] = []  # (source, target, weight)
        self.adjacency_matrix: Dict[str, Dict[str, float]] = {}

    def add_vertex(self, subsystem: Subsystem):
        """Add a subsystem vertex to the graph."""
        self.vertices[subsystem.subsystem_id] = subsystem
        if subsystem.subsystem_id not in self.adjacency_matrix:
            self.adjacency_matrix[subsystem.subsystem_id] = {}

    def add_edge(self, source_id: str, target_id: str, weight: float = 1.0):
        """Add a functional coupling edge."""
        self.edges.append((source_id, target_id, weight))
        if source_id not in self.adjacency_matrix:
            self.adjacency_matrix[source_id] = {}
        self.adjacency_matrix[source_id][target_id] = weight

    def get_coupling_strength(self, source_id: str, target_id: str) -> float:
        """Get coupling strength between two subsystems."""
        return self.adjacency_matrix.get(source_id, {}).get(target_id, 0.0)

    def get_dependencies(self, subsystem_id: str) -> List[str]:
        """Get all dependencies for a subsystem."""
        return list(self.adjacency_matrix.get(subsystem_id, {}).keys())

    def detect_cycles(self) -> List[List[str]]:
        """Detect dependency cycles in the graph."""
        cycles = []
        visited = set()
        rec_stack = set()

        def dfs(node: str, path: List[str]):
            visited.add(node)
            rec_stack.add(node)
            path.append(node)

            for neighbor in self.get_dependencies(node):
                if neighbor not in visited:
                    dfs(neighbor, path[:])
                elif neighbor in rec_stack:
                    # Found a cycle
                    cycle_start = path.index(neighbor)
                    cycles.append(path[cycle_start:] + [neighbor])

            rec_stack.remove(node)

        for vertex_id in self.vertices:
            if vertex_id not in visited:
                dfs(vertex_id, [])

        return cycles


class VulnerabilityDiscoveryEngine:
    """QRATUM Discovery Engine for defensive vulnerability analysis."""

    def __init__(self):
        self.subsystems: Dict[str, Subsystem] = {}
        self.interfaces: Dict[str, Interface] = {}
        self.dependency_graph = DependencyGraph()
        self.metrics_history: List[VulnerabilityMetricsReport] = []
        self.coupling_history: Dict[Tuple[str, str], List[Tuple[float, float]]] = {}

    # ==========================================================================
    # I. System Formalization
    # ==========================================================================

    def enumerate_subsystem(self, subsystem: Subsystem):
        """Enumerate a subsystem in the QRATUM stack."""
        self.subsystems[subsystem.subsystem_id] = subsystem
        self.dependency_graph.add_vertex(subsystem)

    def register_interface(self, interface: Interface):
        """Register an interface between subsystems."""
        self.interfaces[interface.interface_id] = interface
        # Add edge to dependency graph with coupling weight
        weight = self._compute_interface_weight(interface)
        self.dependency_graph.add_edge(
            interface.source_subsystem, interface.target_subsystem, weight
        )

    def construct_dependency_graph(self) -> DependencyGraph:
        """Construct complete dependency graph G(V,E) with functional coupling."""
        return self.dependency_graph

    def _compute_interface_weight(self, interface: Interface) -> float:
        """Compute coupling weight for an interface."""
        # Weight based on interface type (tighter coupling = higher weight)
        weights = {
            InterfaceType.SHARED_MEMORY: 1.0,
            InterfaceType.RPC_CALL: 0.8,
            InterfaceType.DATA_FLOW: 0.6,
            InterfaceType.CONTROL_FLOW: 0.7,
            InterfaceType.EVENT_BUS: 0.4,
            InterfaceType.MESSAGE_QUEUE: 0.5,
        }
        return weights.get(interface.interface_type, 0.5)

    # ==========================================================================
    # II. Latent Risk Field Construction
    # ==========================================================================

    def compute_latent_risk_metrics(self, subsystem_id: str) -> LatentRiskMetrics:
        """Compute latent risk metrics for a subsystem node.

        Computes:
        - Defect density (ρ)
        - Resilience elasticity (Er)
        - Coupling volatility (σC)
        - Recovery half-life (τ)
        """
        subsystem = self.subsystems.get(subsystem_id)
        if not subsystem:
            raise ValueError(f"Subsystem {subsystem_id} not found")

        # Compute defect density (ρ)
        defect_density = self._compute_defect_density(subsystem)

        # Compute resilience elasticity (Er)
        resilience_elasticity = self._compute_resilience_elasticity(subsystem)

        # Compute coupling volatility (σC)
        coupling_volatility = self._compute_coupling_volatility(subsystem_id)

        # Compute recovery half-life (τ)
        recovery_half_life = self._compute_recovery_half_life(subsystem)

        # Compute local entropy
        entropy_local = self._compute_local_entropy(subsystem_id)

        # Compute average entropy
        entropy_average = self._compute_average_entropy()

        return LatentRiskMetrics(
            defect_density=defect_density,
            resilience_elasticity=resilience_elasticity,
            coupling_volatility=coupling_volatility,
            recovery_half_life=recovery_half_life,
            entropy_local=entropy_local,
            entropy_average=entropy_average,
        )

    def _compute_defect_density(self, subsystem: Subsystem) -> float:
        """Compute latent defect density (ρ) for a subsystem.

        Based on:
        - Complexity (number of interfaces and dependencies)
        - Subsystem type risk profile
        - Historical metrics if available
        """
        complexity = len(subsystem.interfaces) + len(subsystem.dependencies)

        # Risk multipliers by subsystem type
        risk_multipliers = {
            SubsystemType.GENOMICS_PIPELINE: 0.8,  # Well-validated domain
            SubsystemType.AION_RUNTIME: 1.2,  # Complex runtime
            SubsystemType.DATA_INGESTION: 0.9,
            SubsystemType.ORCHESTRATION: 1.3,  # High complexity
            SubsystemType.GOVERNANCE: 1.1,
            SubsystemType.ECONOMIC_CONTROL: 1.0,
            SubsystemType.MERKLE_CHAIN: 0.7,  # Well-understood
            SubsystemType.CONTRACT_ENGINE: 1.0,
            SubsystemType.QUANTUM_KERNEL: 1.5,  # Bleeding edge
            SubsystemType.MEMORY_MANAGER: 1.1,
            SubsystemType.SCHEDULER: 1.2,
        }

        base_density = 0.1 * complexity
        multiplier = risk_multipliers.get(subsystem.subsystem_type, 1.0)

        return base_density * multiplier

    def _compute_resilience_elasticity(self, subsystem: Subsystem) -> float:
        """Compute resilience elasticity (Er) - ability to recover from perturbations.

        Higher values indicate better resilience.
        """
        # Factors: redundancy, isolation, recovery mechanisms
        dependency_count = len(subsystem.dependencies)

        # More dependencies = lower resilience
        base_resilience = 1.0 if dependency_count == 0 else 1.0 / (1.0 + 0.1 * dependency_count)

        # Critical subsystems get resilience boost (assumed to have better design)
        critical_types = {
            SubsystemType.MERKLE_CHAIN,
            SubsystemType.CONTRACT_ENGINE,
            SubsystemType.GOVERNANCE,
        }

        if subsystem.subsystem_type in critical_types:
            base_resilience *= 1.2

        return min(base_resilience, 1.0)

    def _compute_coupling_volatility(self, subsystem_id: str) -> float:
        """Compute coupling volatility (σC) - variance in coupling strength."""
        dependencies = self.dependency_graph.get_dependencies(subsystem_id)

        if not dependencies:
            return 0.0

        # Get coupling strengths
        strengths = []
        for dep_id in dependencies:
            strength = self.dependency_graph.get_coupling_strength(subsystem_id, dep_id)
            strengths.append(strength)

        # Compute variance
        if len(strengths) < 2:
            return 0.0

        mean = sum(strengths) / len(strengths)
        variance = sum((s - mean) ** 2 for s in strengths) / len(strengths)

        return math.sqrt(variance)

    def _compute_recovery_half_life(self, subsystem: Subsystem) -> float:
        """Compute recovery half-life (τ) in seconds.

        Time for system to recover 50% after disruption.
        """
        # Based on subsystem type and complexity
        base_half_lives = {
            SubsystemType.GENOMICS_PIPELINE: 300.0,  # 5 minutes
            SubsystemType.AION_RUNTIME: 60.0,  # 1 minute
            SubsystemType.DATA_INGESTION: 30.0,  # 30 seconds
            SubsystemType.ORCHESTRATION: 120.0,  # 2 minutes
            SubsystemType.GOVERNANCE: 180.0,  # 3 minutes
            SubsystemType.ECONOMIC_CONTROL: 240.0,  # 4 minutes
            SubsystemType.MERKLE_CHAIN: 10.0,  # 10 seconds (fast recovery)
            SubsystemType.CONTRACT_ENGINE: 15.0,  # 15 seconds
            SubsystemType.QUANTUM_KERNEL: 600.0,  # 10 minutes
            SubsystemType.MEMORY_MANAGER: 5.0,  # 5 seconds
            SubsystemType.SCHEDULER: 20.0,  # 20 seconds
        }

        base = base_half_lives.get(subsystem.subsystem_type, 60.0)

        # Adjust for complexity
        complexity_factor = 1.0 + 0.1 * len(subsystem.dependencies)

        return base * complexity_factor

    # ==========================================================================
    # III. Entropy & Blind-Zone Detection
    # ==========================================================================

    def _compute_local_entropy(self, subsystem_id: str) -> float:
        """Compute local entropy for a subsystem.

        Measures uncertainty in:
        - Data flow patterns
        - Execution scheduling
        - Decision paths
        """
        subsystem = self.subsystems[subsystem_id]

        # Entropy based on branching factor and state space
        num_interfaces = len(subsystem.interfaces)
        num_dependencies = len(subsystem.dependencies)

        if num_interfaces == 0:
            return 0.0

        # Shannon entropy approximation
        # H = -Σ(p_i * log(p_i)) where p_i = 1/n for uniform distribution
        n = num_interfaces + num_dependencies
        if n <= 1:
            return 0.0

        p = 1.0 / n
        entropy = -n * p * math.log2(p)

        return entropy

    def _compute_average_entropy(self) -> float:
        """Compute average entropy across all subsystems."""
        if not self.subsystems:
            return 0.0

        total_entropy = sum(self._compute_local_entropy(sid) for sid in self.subsystems)

        return total_entropy / len(self.subsystems)

    def detect_entropy_bottlenecks(self) -> List[EntropyBottleneck]:
        """Detect structural blind-zones where H_local >> H̄."""
        bottlenecks = []
        avg_entropy = self._compute_average_entropy()

        if avg_entropy == 0:
            return bottlenecks

        for subsystem_id, subsystem in self.subsystems.items():
            local_entropy = self._compute_local_entropy(subsystem_id)
            ratio = local_entropy / avg_entropy if avg_entropy > 0 else 0

            # Flag if local entropy significantly exceeds average
            if ratio > 1.5:  # 50% above average
                severity = self._classify_entropy_severity(ratio)
                bottleneck_type = self._classify_bottleneck_type(subsystem)

                bottlenecks.append(
                    EntropyBottleneck(
                        subsystem_id=subsystem_id,
                        entropy_ratio=ratio,
                        bottleneck_type=bottleneck_type,
                        severity=severity,
                        description=f"{subsystem.name} has {ratio:.2f}x average entropy",
                    )
                )

        return bottlenecks

    def _classify_entropy_severity(self, ratio: float) -> str:
        """Classify entropy bottleneck severity."""
        if ratio > 3.0:
            return "CRITICAL"
        elif ratio > 2.5:
            return "HIGH"
        elif ratio > 2.0:
            return "MEDIUM"
        else:
            return "LOW"

    def _classify_bottleneck_type(self, subsystem: Subsystem) -> str:
        """Classify type of entropy bottleneck."""
        type_mapping = {
            SubsystemType.GENOMICS_PIPELINE: "genomic_inference",
            SubsystemType.DATA_INGESTION: "data_flow",
            SubsystemType.SCHEDULER: "execution_scheduling",
            SubsystemType.GOVERNANCE: "governance_decision",
            SubsystemType.ORCHESTRATION: "execution_scheduling",
        }
        return type_mapping.get(subsystem.subsystem_type, "data_flow")

    # ==========================================================================
    # IV. Coupling Drift Surveillance
    # ==========================================================================

    def track_coupling_drift(self, timestamp: Optional[str] = None) -> List[CouplingDrift]:
        """Track time-derivatives of inter-module coupling dC/dt."""
        if timestamp is None:
            timestamp = datetime.utcnow().isoformat()

        drifts = []

        for source_id, targets in self.dependency_graph.adjacency_matrix.items():
            for target_id, coupling_strength in targets.items():
                # Get coupling history
                key = (source_id, target_id)
                if key not in self.coupling_history:
                    self.coupling_history[key] = []

                # Compute drift rate
                history = self.coupling_history[key]
                if len(history) >= 2:
                    # dC/dt ≈ (C_now - C_prev) / Δt
                    prev_time, prev_coupling = history[-1]
                    current_time = datetime.fromisoformat(timestamp).timestamp()
                    prev_timestamp = prev_time

                    dt = current_time - prev_timestamp
                    drift_rate = (coupling_strength - prev_coupling) / dt if dt > 0 else 0.0
                else:
                    drift_rate = 0.0

                # Record current coupling
                current_time = datetime.fromisoformat(timestamp).timestamp()
                self.coupling_history[key].append((current_time, coupling_strength))

                # Detect metastable clusters (super-linear risk amplification)
                is_metastable = self._is_metastable_coupling(source_id, target_id, drift_rate)

                drifts.append(
                    CouplingDrift(
                        source_id=source_id,
                        target_id=target_id,
                        coupling_strength=coupling_strength,
                        drift_rate=drift_rate,
                        timestamp=timestamp,
                        is_metastable=is_metastable,
                    )
                )

        return drifts

    def _is_metastable_coupling(self, source_id: str, target_id: str, drift_rate: float) -> bool:
        """Detect metastable clusters with super-linear risk amplification."""
        # Check if drift rate is accelerating
        key = (source_id, target_id)
        history = self.coupling_history.get(key, [])

        if len(history) < 3:
            return False

        # Check if coupling is increasing rapidly
        recent_couplings = [c for _, c in history[-3:]]
        if len(recent_couplings) >= 3:
            # Check for super-linear growth
            diff1 = recent_couplings[1] - recent_couplings[0]
            diff2 = recent_couplings[2] - recent_couplings[1]

            if diff2 > diff1 * 1.5:  # 50% acceleration
                return True

        return abs(drift_rate) > 0.1  # Arbitrary threshold

    # ==========================================================================
    # V. Phase-Transition Modeling
    # ==========================================================================

    def model_phase_transition(self) -> PhaseTransitionRisk:
        """Model system health and detect collapse precursors.

        System health: m(t) = 1 - L(t)/L_crit
        where L(t) is current load and L_crit is critical load.
        """
        # Compute aggregate load metric
        total_defect_density = 0.0
        total_coupling = 0.0
        num_subsystems = len(self.subsystems)

        if num_subsystems == 0:
            return PhaseTransitionRisk(
                system_health=1.0,
                variance_spike=False,
                entropy_localization=False,
                resilience_compression=False,
                collapse_probability=0.0,
                precursor_signals=[],
            )

        for subsystem_id in self.subsystems:
            metrics = self.compute_latent_risk_metrics(subsystem_id)
            total_defect_density += metrics.defect_density

            # Sum coupling weights
            for _, _, weight in self.dependency_graph.edges:
                total_coupling += weight

        # Normalize load
        avg_defect_density = total_defect_density / num_subsystems

        # Define critical thresholds
        # Mathematical notation from problem statement (L_crit)
        L_crit_defects = 5.0  # noqa: N806  # Critical defect density
        L_crit_coupling = num_subsystems * 2.0  # noqa: N806  # Critical coupling threshold

        # Compute load ratio
        # Mathematical notation from problem statement (L)
        L_defects = avg_defect_density / L_crit_defects  # noqa: N806
        L_coupling = total_coupling / L_crit_coupling if L_crit_coupling > 0 else 0  # noqa: N806
        L_total = (L_defects + L_coupling) / 2.0  # noqa: N806

        # System health: m(t) = 1 - L(t)/L_crit
        system_health = max(0.0, 1.0 - L_total)

        # Detect precursor signals
        precursor_signals = []

        # 1. Variance spikes
        variance_spike = self._detect_variance_spike()
        if variance_spike:
            precursor_signals.append("Variance spike detected in coupling metrics")

        # 2. Entropy localization
        entropy_localization = self._detect_entropy_localization()
        if entropy_localization:
            precursor_signals.append("Entropy localization detected in critical subsystems")

        # 3. Resilience half-life compression
        resilience_compression = self._detect_resilience_compression()
        if resilience_compression:
            precursor_signals.append("Resilience half-life compression detected")

        # Compute collapse probability
        collapse_probability = self._compute_collapse_probability(
            system_health, variance_spike, entropy_localization, resilience_compression
        )

        return PhaseTransitionRisk(
            system_health=system_health,
            variance_spike=variance_spike,
            entropy_localization=entropy_localization,
            resilience_compression=resilience_compression,
            collapse_probability=collapse_probability,
            precursor_signals=precursor_signals,
        )

    def _detect_variance_spike(self) -> bool:
        """Detect variance spikes in coupling metrics."""
        if not self.coupling_history:
            return False

        # Compute variance of recent coupling changes
        recent_changes = []
        for _key, history in self.coupling_history.items():
            if len(history) >= 2:
                last_coupling = history[-1][1]
                prev_coupling = history[-2][1]
                change = abs(last_coupling - prev_coupling)
                recent_changes.append(change)

        if not recent_changes:
            return False

        mean_change = sum(recent_changes) / len(recent_changes)
        variance = sum((c - mean_change) ** 2 for c in recent_changes) / len(recent_changes)

        # Threshold for spike detection
        return variance > 0.1

    def _detect_entropy_localization(self) -> bool:
        """Detect entropy localization in critical subsystems."""
        bottlenecks = self.detect_entropy_bottlenecks()

        # Check if critical subsystems have high entropy
        critical_types = {
            SubsystemType.MERKLE_CHAIN,
            SubsystemType.CONTRACT_ENGINE,
            SubsystemType.GOVERNANCE,
        }

        for bottleneck in bottlenecks:
            subsystem = self.subsystems.get(bottleneck.subsystem_id)
            if (
                subsystem
                and subsystem.subsystem_type in critical_types
                and bottleneck.severity in ["HIGH", "CRITICAL"]
            ):
                return True

        return False

    def _detect_resilience_compression(self) -> bool:
        """Detect resilience half-life compression."""
        if len(self.metrics_history) < 2:
            return False

        # Compare recent metrics to historical baseline
        recent_metrics = self.metrics_history[-1].metrics
        baseline_metrics = self.metrics_history[0].metrics

        compression_count = 0
        for subsystem_id in recent_metrics:
            if subsystem_id in baseline_metrics:
                recent_tau = recent_metrics[subsystem_id].recovery_half_life
                baseline_tau = baseline_metrics[subsystem_id].recovery_half_life

                # Check if recovery time has increased significantly
                if recent_tau > baseline_tau * 1.5:
                    compression_count += 1

        # If more than 30% of subsystems show compression
        return compression_count > len(recent_metrics) * 0.3

    def _compute_collapse_probability(
        self,
        system_health: float,
        variance_spike: bool,
        entropy_localization: bool,
        resilience_compression: bool,
    ) -> float:
        """Compute probability of system collapse."""
        # Base probability from system health
        base_prob = max(0.0, 1.0 - system_health)

        # Multiply by precursor signal factors
        multiplier = 1.0
        if variance_spike:
            multiplier *= 1.5
        if entropy_localization:
            multiplier *= 1.8
        if resilience_compression:
            multiplier *= 1.6

        collapse_prob = min(1.0, base_prob * multiplier)

        return collapse_prob

    # ==========================================================================
    # VI. Metrics Output
    # ==========================================================================

    def generate_metrics_report(self) -> VulnerabilityMetricsReport:
        """Generate complete vulnerability metrics report."""
        timestamp = datetime.utcnow().isoformat()

        # Compute metrics for all subsystems
        metrics = {}
        for subsystem_id in self.subsystems:
            metrics[subsystem_id] = self.compute_latent_risk_metrics(subsystem_id)

        # Detect entropy bottlenecks
        entropy_bottlenecks = self.detect_entropy_bottlenecks()

        # Track coupling drift
        coupling_drifts = self.track_coupling_drift(timestamp)

        # Model phase transition
        phase_risk = self.model_phase_transition()

        # Extract invariant signatures
        invariant_signatures = self.extract_invariant_signatures()

        # Generate mitigation recommendations
        mitigation_recommendations = self.generate_mitigation_architecture(
            metrics, entropy_bottlenecks, coupling_drifts, phase_risk
        )

        report = VulnerabilityMetricsReport(
            timestamp=timestamp,
            subsystems=list(self.subsystems.values()),
            metrics=metrics,
            entropy_bottlenecks=entropy_bottlenecks,
            coupling_drifts=coupling_drifts,
            phase_risk=phase_risk,
            invariant_signatures=invariant_signatures,
            mitigation_recommendations=mitigation_recommendations,
        )

        # Store in history
        self.metrics_history.append(report)

        return report

    def format_metrics_table(self, report: VulnerabilityMetricsReport) -> str:
        """Format metrics report as a table."""
        lines = []
        lines.append("=" * 120)
        lines.append("QRATUM VULNERABILITY METRICS REPORT")
        lines.append(f"Timestamp: {report.timestamp}")
        lines.append("=" * 120)
        lines.append("")

        # Subsystem metrics table
        lines.append("SUBSYSTEM METRICS:")
        lines.append("-" * 120)
        header = f"{'Subsystem':<30} {'ρ':<10} {'Er':<10} {'σC':<10} {'τ (s)':<10} {'H_local':<10} {'Entropy BN':<15} {'Phase Risk':<10}"
        lines.append(header)
        lines.append("-" * 120)

        for subsystem in report.subsystems:
            sid = subsystem.subsystem_id
            metrics = report.metrics.get(sid)

            if metrics:
                # Check for entropy bottleneck
                is_bottleneck = any(bn.subsystem_id == sid for bn in report.entropy_bottlenecks)
                bottleneck_str = "YES" if is_bottleneck else "NO"

                # Phase risk indicator
                if report.phase_risk.system_health > 0.7:
                    phase_risk_str = "LOW"
                elif report.phase_risk.system_health > 0.4:
                    phase_risk_str = "MEDIUM"
                else:
                    phase_risk_str = "HIGH"

                row = (
                    f"{subsystem.name[:28]:<30} "
                    f"{metrics.defect_density:<10.3f} "
                    f"{metrics.resilience_elasticity:<10.3f} "
                    f"{metrics.coupling_volatility:<10.3f} "
                    f"{metrics.recovery_half_life:<10.1f} "
                    f"{metrics.entropy_local:<10.3f} "
                    f"{bottleneck_str:<15} "
                    f"{phase_risk_str:<10}"
                )
                lines.append(row)

        lines.append("-" * 120)
        lines.append("")

        # Entropy bottlenecks
        if report.entropy_bottlenecks:
            lines.append("ENTROPY BOTTLENECKS:")
            lines.append("-" * 80)
            for bn in report.entropy_bottlenecks:
                subsystem = self.subsystems.get(bn.subsystem_id)
                name = subsystem.name if subsystem else bn.subsystem_id
                lines.append(
                    f"  [{bn.severity}] {name}: {bn.description} (type: {bn.bottleneck_type})"
                )
            lines.append("")

        # Coupling drift
        metastable_drifts = [d for d in report.coupling_drifts if d.is_metastable]
        if metastable_drifts:
            lines.append("METASTABLE COUPLING CLUSTERS:")
            lines.append("-" * 80)
            for drift in metastable_drifts:
                src = self.subsystems.get(drift.source_id)
                tgt = self.subsystems.get(drift.target_id)
                src_name = src.name if src else drift.source_id
                tgt_name = tgt.name if tgt else drift.target_id
                lines.append(f"  {src_name} → {tgt_name}: drift_rate={drift.drift_rate:.4f}")
            lines.append("")

        # Phase transition risk
        lines.append("PHASE TRANSITION RISK:")
        lines.append("-" * 80)
        lines.append(f"  System Health: {report.phase_risk.system_health:.3f}")
        lines.append(f"  Collapse Probability: {report.phase_risk.collapse_probability:.3f}")
        lines.append("  Precursor Signals:")
        for signal in report.phase_risk.precursor_signals:
            lines.append(f"    - {signal}")
        if not report.phase_risk.precursor_signals:
            lines.append("    - None detected")
        lines.append("")

        # Invariant signatures
        if report.invariant_signatures:
            lines.append("INVARIANT FAILURE SIGNATURES:")
            lines.append("-" * 80)
            for signature in report.invariant_signatures:
                lines.append(f"  - {signature}")
            lines.append("")

        # Mitigation recommendations
        if report.mitigation_recommendations:
            lines.append("DEFENSIVE MITIGATION RECOMMENDATIONS:")
            lines.append("-" * 80)
            for i, rec in enumerate(report.mitigation_recommendations, 1):
                lines.append(f"  {i}. {rec}")
            lines.append("")

        lines.append("=" * 120)

        return "\n".join(lines)

    # ==========================================================================
    # VII. Invariant Failure Signatures
    # ==========================================================================

    def extract_invariant_signatures(self) -> List[str]:
        """Extract domain-independent fingerprints of degradation."""
        signatures = []

        # Detect dependency cycles
        cycles = self.dependency_graph.detect_cycles()
        if cycles:
            signatures.append(f"Dependency cycles detected: {len(cycles)} cycles")

        # High defect density subsystems
        high_defect_subsystems = []
        for subsystem_id, subsystem in self.subsystems.items():
            metrics = self.compute_latent_risk_metrics(subsystem_id)
            if metrics.defect_density > 3.0:
                high_defect_subsystems.append(subsystem.name)

        if high_defect_subsystems:
            signatures.append(f"High defect density in: {', '.join(high_defect_subsystems)}")

        # Low resilience subsystems
        low_resilience_subsystems = []
        for subsystem_id, subsystem in self.subsystems.items():
            metrics = self.compute_latent_risk_metrics(subsystem_id)
            if metrics.resilience_elasticity < 0.3:
                low_resilience_subsystems.append(subsystem.name)

        if low_resilience_subsystems:
            signatures.append(f"Low resilience in: {', '.join(low_resilience_subsystems)}")

        # High coupling volatility
        high_volatility_subsystems = []
        for subsystem_id, subsystem in self.subsystems.items():
            metrics = self.compute_latent_risk_metrics(subsystem_id)
            if metrics.coupling_volatility > 0.5:
                high_volatility_subsystems.append(subsystem.name)

        if high_volatility_subsystems:
            signatures.append(
                f"High coupling volatility in: {', '.join(high_volatility_subsystems)}"
            )

        return signatures

    # ==========================================================================
    # VIII. Defensive Mitigation Architecture
    # ==========================================================================

    def generate_mitigation_architecture(
        self,
        metrics: Dict[str, LatentRiskMetrics],
        entropy_bottlenecks: List[EntropyBottleneck],
        coupling_drifts: List[CouplingDrift],
        phase_risk: PhaseTransitionRisk,
    ) -> List[str]:
        """Generate defensive mitigation recommendations."""
        recommendations = []

        # Design-time immunization doctrine
        high_defect_systems = [sid for sid, m in metrics.items() if m.defect_density > 2.0]
        if high_defect_systems:
            for sid in high_defect_systems:
                subsystem = self.subsystems.get(sid)
                if subsystem:
                    recommendations.append(
                        f"Design-time: Reduce complexity in {subsystem.name} "
                        f"(current defect density: {metrics[sid].defect_density:.2f})"
                    )

        # Continuous sensing layer specification
        if entropy_bottlenecks:
            critical_bottlenecks = [
                bn for bn in entropy_bottlenecks if bn.severity in ["HIGH", "CRITICAL"]
            ]
            if critical_bottlenecks:
                for bn in critical_bottlenecks:
                    subsystem = self.subsystems.get(bn.subsystem_id)
                    if subsystem:
                        recommendations.append(
                            f"Continuous sensing: Monitor {subsystem.name} for "
                            f"{bn.bottleneck_type} anomalies (entropy ratio: {bn.entropy_ratio:.2f})"
                        )

        # Entropy budgeting framework
        avg_entropy = (
            sum(m.entropy_local for m in metrics.values()) / len(metrics) if metrics else 0
        )
        for sid, m in metrics.items():
            if m.entropy_local > avg_entropy * 2.0:
                subsystem = self.subsystems.get(sid)
                if subsystem:
                    recommendations.append(
                        f"Entropy budget: Implement complexity limits for {subsystem.name} "
                        f"(current: {m.entropy_local:.2f}, budget: {avg_entropy * 1.5:.2f})"
                    )

        # Epistemic calibration loops
        metastable_drifts = [d for d in coupling_drifts if d.is_metastable]
        if metastable_drifts:
            for drift in metastable_drifts[:3]:  # Top 3
                src = self.subsystems.get(drift.source_id)
                tgt = self.subsystems.get(drift.target_id)
                if src and tgt:
                    recommendations.append(
                        f"Epistemic calibration: Review coupling between "
                        f"{src.name} and {tgt.name} (drift rate: {drift.drift_rate:.4f})"
                    )

        # Phase transition mitigation
        if phase_risk.collapse_probability > 0.3:
            recommendations.append(
                f"URGENT: System collapse risk at {phase_risk.collapse_probability:.1%}. "
                f"Implement emergency load shedding and graceful degradation."
            )

        # Resilience enhancement
        low_resilience_systems = [
            sid for sid, m in metrics.items() if m.resilience_elasticity < 0.4
        ]
        if low_resilience_systems:
            for sid in low_resilience_systems[:3]:  # Top 3
                subsystem = self.subsystems.get(sid)
                if subsystem:
                    recommendations.append(
                        f"Resilience: Add redundancy and isolation to {subsystem.name} "
                        f"(current Er: {metrics[sid].resilience_elasticity:.2f})"
                    )

        # Recovery optimization
        slow_recovery_systems = [
            sid for sid, m in metrics.items() if m.recovery_half_life > 300.0  # > 5 minutes
        ]
        if slow_recovery_systems:
            for sid in slow_recovery_systems[:3]:  # Top 3
                subsystem = self.subsystems.get(sid)
                if subsystem:
                    recommendations.append(
                        f"Recovery: Optimize recovery procedures for {subsystem.name} "
                        f"(current τ: {metrics[sid].recovery_half_life:.1f}s)"
                    )

        return recommendations

    def export_report_json(self, report: VulnerabilityMetricsReport, filepath: str):
        """Export report to JSON file."""
        # Convert report to dict
        report_dict = {
            "timestamp": report.timestamp,
            "subsystems": [
                {
                    "id": s.subsystem_id,
                    "name": s.name,
                    "type": s.subsystem_type.value,
                    "description": s.description,
                    "dependencies": s.dependencies,
                }
                for s in report.subsystems
            ],
            "metrics": {
                sid: {
                    "defect_density": m.defect_density,
                    "resilience_elasticity": m.resilience_elasticity,
                    "coupling_volatility": m.coupling_volatility,
                    "recovery_half_life": m.recovery_half_life,
                    "entropy_local": m.entropy_local,
                    "entropy_average": m.entropy_average,
                }
                for sid, m in report.metrics.items()
            },
            "entropy_bottlenecks": [
                {
                    "subsystem_id": bn.subsystem_id,
                    "entropy_ratio": bn.entropy_ratio,
                    "type": bn.bottleneck_type,
                    "severity": bn.severity,
                    "description": bn.description,
                }
                for bn in report.entropy_bottlenecks
            ],
            "coupling_drifts": [
                {
                    "source_id": d.source_id,
                    "target_id": d.target_id,
                    "coupling_strength": d.coupling_strength,
                    "drift_rate": d.drift_rate,
                    "is_metastable": d.is_metastable,
                }
                for d in report.coupling_drifts
            ],
            "phase_risk": {
                "system_health": report.phase_risk.system_health,
                "variance_spike": report.phase_risk.variance_spike,
                "entropy_localization": report.phase_risk.entropy_localization,
                "resilience_compression": report.phase_risk.resilience_compression,
                "collapse_probability": report.phase_risk.collapse_probability,
                "precursor_signals": report.phase_risk.precursor_signals,
            },
            "invariant_signatures": report.invariant_signatures,
            "mitigation_recommendations": report.mitigation_recommendations,
        }

        try:
            with open(filepath, "w") as f:
                json.dump(report_dict, f, indent=2)
        except OSError as e:
            raise RuntimeError(f"Failed to export report to {filepath}: {e}") from e
